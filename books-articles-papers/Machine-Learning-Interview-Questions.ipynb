{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the difference between AI and ML?\n",
    "\n",
    "AI stands for Artificial Intelligence and ML stands for Machine Learning. These are the two hot buzz words in today’s era. This is the decade of big data. What makes a big data special is the humongous data set which are provided for analysis. These data sets cannot be processed using rudiment database management system on the first place.\n",
    "\n",
    "AI was introduced in 1960 with the purpose to help human by creating computer programs which can replicate human actions in the same way for a given situation. It was supposed to have intelligence at par with human.\n",
    "\n",
    "Machine learning on the other hand is just one aspect of AI. Given a scenario, a machine should be able to learn prediction. Given a data set the machine should be able to learn from the trend/past behavior and predict the answer. If I give dataset for Man of the Match in a set of cricket matches, then the machine should be able to come with the right candidate for Man of The Match in the coming matches. The better the learning curve of the model, the better is the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is deep learning?\n",
    "\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is gradient descent?\n",
    "\n",
    "Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on a convex function, that tweaks it’s parameters iteratively to minimize a given function to its local minimum.\n",
    "\n",
    "It is simply used to find the values of a functions parameters (coefficients) that minimize a cost function as far as possible.\n",
    "\n",
    "“A gradient measures how much the output of a function changes if you change the inputs a little bit.” — Lex Fridman (MIT)\n",
    "\n",
    "It simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. Said it more mathematically, a gradient is a partial derivative with respect to its inputs.\n",
    "\n",
    "Gradient Descent can be thought of climbing down to the bottom of a valley, instead of climbing up a hill. This is because it is a minimization algorithm that minimizes a given function.\n",
    "\n",
    "The equation below describes what Gradient Descent does: \"b“ describes the next position of our climber, while \"a“ represents his current position. The minus sign refers to the minimization part of gradient descent. The \"gamma“ in the middle is a waiting factor and the gradient term ( Δf(a) ) is simply the direction of the steepest descent.\n",
    "\n",
    "![Fig](imgs/img_001.png)\n",
    "\n",
    "[Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What is backpropagation?\n",
    "\n",
    "Backpropagation is a technique which helps us in tuning our model by changing the parameters involved in the model until a better accuracy is achieved.\n",
    " \n",
    "In layman term, suppose you have to create and ensemble model which needs to have weight for every algorithm which you have selected for the model. Backpropagation will just increase/decrease both the model weightage as well as the parameter of each model to achieve a better accuracy, precision, recall and F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision comes up from the English word precise which means accurate.\n",
    " \n",
    "Take an example – Your mother has gifted you 100 things in the past 10 years. She loves you and you love her back. One fine day she comes up and asks you “Do you remember all my 100 gifts I gave you?”\n",
    " \n",
    "You can’t run away, but you can try recalling. With God’s grace you were able to remember 80 items, not good but you did decent.\n",
    "\n",
    "Your recall rate here is 80%, all the items you remembered were correct, so the precision here will be 100%.\n",
    "\n",
    "p is the number of correct postive results divided by total number of postive results.\n",
    "\n",
    "Take another example where you had 3 girlfriends(highly hypothetical), and the current girlfriend asked you to recall all the 10 gifts she had given you in the past 2 years.\n",
    " \n",
    "You tried and answered all 10, but to your surprise only 7 were from this girlfriend..oopss!!\n",
    " \n",
    "So, your recall percentage is 100 but the precision is 70%\n",
    " \n",
    "r is the number of correct positive results divided by the number of correct positive results and incorrect negative results (or postive results that should have been returned)\n",
    "\n",
    "![Fig](imgs/img_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is F-score? Is it like the perfect metric to judge the performance of a model?\n",
    "\n",
    "A.   It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What is the most important parameter to judge a model?\n",
    "A. To be frank, there are many. You can’t judge any model just on accuracy, precision, recall, Area Under Curve, Sensitivity, specificity, F-score. The final model or the ensemble model has to perform equally well on multiple parameters.\n",
    " \n",
    "Ideally a model with good accuracy in train data set and almost comparable accuracy in validation data set is good. It should have a high Area Under Curve and F-score\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What are the type of datasets?\n",
    "A. Basically there is just dataset. Say suppose, you have the dataset for diabetes patient. Your dataset contains Height, weight, insulin level, age, diabetes(true or false)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. What is train and validation dataset?\n",
    "A. Say your dataset has 10000 rows. Now you have 10000 rows, you want to build a model. You take all the rows and feed in your model. Your accuracy and everything is very good. But how will you validate the model?\n",
    " \n",
    "To answer this question, we divide the 10000 rows into two parts – Train and Validation in the ratio 70:30 or 80:20 .\n",
    " \n",
    "You can build your model with 70 or 80 % of the data and the rest 20 % will be there to validate your model.\n",
    "\n",
    "Example – Your diabetes train data set got an accuracy of 90% but when you tested this model with the validate dataset the accuracy was ~50. This means that model will not work well for other datasets.\n",
    " \n",
    "If the accuracy is almost the same as train dataset then it’s a decent model. Again one have to look for good F-score and AUC.\n",
    " \n",
    "To keep it short, a good model depends on many parameters which varies from one problem statement to another\n",
    "If the accuracy is almost the same as train dataset then it’s a decent model. Again one have to look for good F-score and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Explain an ROC curve.\n",
    " \n",
    "A. Receiver Operating Curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).\n",
    " \n",
    "For a binary classifier, the ROC curve plots the true positive rate versus the fall positive rate, over a varying threshold. For example, say you developed some test for a disease. You would obtain some data on patients with the disease, and control subjects, then fit a model on it; the purpose of the model is to predict disease status from some set of variables. The true positive rate is subjects who have the disease who are correctly identified as having it, and the false positive rate is subjects who don’t have the disease, but are identified as having it (by your model).\n",
    "\n",
    "There is some threshold for designating that a person has the disease. Say at or above a test value of .5, you consider that enough evidence to denote that person as having the disease. As you vary this threshold, the true and false positive rates change. This is the ROC curve.\n",
    "The most concise interpretation in my opinion, is the AUC, the area under the (ROC) curve. It ranges from 0.5 (classification at random) to 1.0 (perfect classification). You can generally think of the AUC as the probability that your model will correctly classify a given subject into one of the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. What’s your favourite algorithm, and can you explain it to me in less than a minute?\n",
    "\n",
    "A. This type of question tests your understanding of how to communicate complex and technical nuances with poise and the ability to summarize quickly and efficiently.\n",
    "You can take an example to show the importance of the algorithm of your choice. Given a chance I would start with linear and logistic and then move to Boosting algorithms like XGBoost or LightGBM or Random forest using a classification problem. \n",
    "\n",
    "There was one problem where we had to determine whether a given patient will be affected by some disease or not. It was a classic classification problem. I started with Linear model, had a decent accuracy in logistic regression, awesome AUC in random forest. So I finally decided to settle with an ensemble model of XGB and Naïve Bayes.\n",
    "\n",
    "Given a chance you should also tell all your favourite algorithm and later boil down to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. What’s the difference between Type I and Type II error?\n",
    "\n",
    "A. Interviewer might try to pin you down with some confusing term, and this one being one of them. Type 1 and Type 2 error is nothing but false positive and false negative.\n",
    "Type 1 means claiming an event has happened but it hasn’t.\n",
    "Type 2 means claiming that nothing has happened when in fact then event has already took place\n",
    "A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. How is a decision tree pruned?\n",
    "A. Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.\n",
    "\n",
    "Reduced error pruning is perhaps the simplest version: replace each node.\n",
    "\n",
    "If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Which is more important to you– model accuracy, or model performance?\n",
    "\n",
    "A. This question tests your grasp of the nuances of machine learning model performance! Machine learning interview questions often look towards the details. There are models with higher accuracy that can perform worse in predictive power — how does that make sense?<br>\n",
    "\n",
    "Well, it has everything to do with how model accuracy is only a subset of model performance, and at that, a sometimes misleading one.<br>\n",
    "\n",
    "For example, if you wanted to detect fraud in a massive dataset with a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast minority of cases were fraud. However, this would be useless for a predictive model — a model designed to find fraud that asserted there was no fraud at all! Questions like this help you demonstrate that you understand model accuracy isn’t the be-all and end-all of model performance.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. How would you handle an imbalanced dataset?\n",
    "A. An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:\n",
    "1. Collect more data to even the imbalances in the dataset.\n",
    "2. Resample the dataset to correct for imbalances.\n",
    "3. Try a different algorithm altogether on your dataset. Many algorithms provide scaling of samples.\n",
    "What’s important here is that you have a keen sense for what damage an unbalanced dataset can cause, and how to balance that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Name an example where ensemble techniques might be useful.\n",
    "A. Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by small changes in the training data). \n",
    "You could list some examples of ensemble methods, the problem of identifying diabetic patient is one such example where you need to have an ensemble method to have a better accuracy and f-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. How do you ensure you’re not overfitting with a model?\n",
    "A. This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations.\n",
    "There are three main methods to avoid overfitting:\n",
    "\n",
    "1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.\n",
    "\n",
    "2- Use cross-validation techniques such as k-folds cross-validation.\n",
    "\n",
    "3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Why is sometimes 96% accuracy is also bad?\n",
    "A. This is really important to understand that accuracy is not the only measure of success for a model. Suppose you are making a model for cancer patients, there an accuracy of even 96% is not that acceptable because if 4% of the people who have been labeled as not infected by cancer later found to be suffering from cancer, then the accuracy of the model won’t matter that much.\n",
    "\n",
    "In this case we are more concerned about sensitivity and specificity of the model\n",
    "Sensitivity is true positive rate and specificity is true negative rate\n",
    " \n",
    "As a remedy we can take the following step to have a better sample for creating the model:-\n",
    " \n",
    "a. Try using under sampling or oversampling method to create your dataset\n",
    "b. We can assign weight to classes such that the minority classes gets larger weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Can you tell me how sensitivity and specificity are inversely proportional to each other?\n",
    "A. “Inversely proportional” means that as one goes up, the other goes down.\n",
    "\n",
    "“Sensitivity” is the proportion of positives that are correctly identified as positives.\n",
    "\n",
    "“Specificity” is the proportion of negatives that are correctly identified as negatives.\n",
    "\n",
    "So, suppose you have some test for a disease and your test gives a numerical score from 0 to 100 with a higher score indicating greater likelihood of disease. You have to set a cutoff (or, possibly, several, or maybe something fuzzy, but let’s keep it simple).\n",
    "If you say “only people with a score of 100 have the disease” then you will have very good sensitivity but horrible specificity.\n",
    "If you say “anyone with a score higher than 1 has the disease” then you will have horrible sensitivity and great specificity.\n",
    "Where to set the cut off depends on the exact shape of the ROC and on the cost of a false negative vs. a false positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. What is the common flow of control while creating a model?\n",
    "\n",
    "A. Common flow of control goes like one below:\n",
    " \n",
    "i.   Get the data<br>\n",
    "ii.  Clean the data<br>\n",
    "iii. Find missing value<br>\n",
    "iv. Perform missing value treatment<br>\n",
    "v.  Find correlation between dependent- dependent and independent-dependent variable<br>\n",
    "vi.  Select the set of variable<br>\n",
    "vii. Divide the data set into train and validation<br>\n",
    "viii. Create your model with train data set<br>\n",
    "ix. Measure accuracy, ROC, accuracy, recall and f-score<br>\n",
    "x. Re-run some other model or create an ensemble model<br>\n",
    "xi. Test the models on validation dataset<br>\n",
    " \n",
    "These are the most common steps we take while creating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. What is cleaning of data?\n",
    "A. So, you have a lot of data, mainly big data with a lot of rows and columns, now before you start building your model, you need to clean your data. There are many ways for it:-\n",
    " \n",
    "a. Remove NULL and zeros (if your dataset is like that which permits removal of these) – You can’t remove zeroes from a cricket player of the match model. You need to know the meaning of the variable and the importance of NULL and zeroes\n",
    " \n",
    "b. You can remove the rows which have a lot of columns with NULL/zeroes\n",
    " \n",
    "c. In order to replace the missing values, you can use Mean, median or mode according to need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. What is mean substitution method\n",
    "\n",
    "A. Mean imputation technique is a standout amongst the most commonly used strategies. Mean substitution replaces missing values on a variable with the mean estimation of the observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. What is median substitution method?\n",
    "A. Mean or median substitution of covariates and result variables is still regularly utilized. The median is essentially as same as the mean. It is determined through gathering of data and calculating average. Median imputation brings about the average of the whole data set being the same as it would be with case deletion, yet the variability between individual’s reactions is diminished, biasing differences and covariance’s toward zero. Since the mean is influenced by the outliers appears common to utilize the median rather to just guarantee power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. You are assigned a project to look after Pizza Hut, their problem is that they are suffering a heavy loss due to late delivery as they have to deliver it free. Which algorithm to look after?\n",
    "A. I would like to solve it using Support Vector Machine(SVM) or Random Forest. Ohh !! Wait, this is not a ML program at all. When ever you need favor from ML, your dataset need to have  the following:-\n",
    " \n",
    "1. Trend in data – In this case there is no such trend.<br>\n",
    "2. Proper data which can go through missing value treatment before processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Why do we need to know the relationship between dependent-dependent and dependent-independent variable?\n",
    "A. A simple thumb rule. If an independent variable have a good dependency on a dependent variable then it should be taken into consideration for the model.\n",
    " \n",
    "If two dependent variables are highly dependent on each other then only one should be considered, as presence both will bias the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Why a model which gives an excellent result to a train dataset, disappoints on a test dataset\n",
    "A.Low bias occurs when the model’s predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it sounds like great achievement, but not to forget, a flexible model has no generalization capabilities. It means, when this model is tested on an unseen data, it gives disappointing results.<br>\n",
    "In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms divides a data set into subsets made with repeated randomized sampling. Then, these samples are used to generate  a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).<br>\n",
    "Also, to combat high variance, we can:<br>\n",
    "Use regularization technique, where higher model coefficients get penalized, hence lowering model complexity.\n",
    "Use top n features from variable importance chart. May be, with all the variable in the data set, the algorithm is having difficulty in finding the meaningful signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. Now, let’s talk about some models. \n",
    "### How can you remove multicollinearity from your model? (We have already discussed it)\n",
    "A. If there is multicollinearity, then somewhere there are a couple or more variables which are highly dependent on each other showing a high correlation.\n",
    " \n",
    "A. You have to look for VIF or take one out of the two correlated variables and test whether the model is working fine with one of the variable, then check for the second one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. How much data should you allocate for your training, validation, and test sets?\n",
    "### You have to find a balance, and there's no right answer for every problem.\n",
    "A. If your test set is too small, you'll have an unreliable estimation of model performance (performance statistic will have high variance). If your training set is too small, your actual model parameters will have high variance.\n",
    "A good rule of thumb is to use an 80/20 train/test split. Your train set can be further split into train/validation or into partitions for cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. If you split your data into train/test splits, is it still possible to overfit your model?\n",
    "A. Yes, it's definitely possible. One common beginner mistake is re-tuning a model or training new models with different parameters after seeing its performance on the test set.\n",
    "In this case, its the model selection process that causes the overfitting. The test set should not be tainted until you're ready to make your final selection.\n",
    "\n",
    "### 31. What are the advantages and disadvantages of decision trees?\n",
    "Advantages: Decision trees are easy to interpret, nonparametric (which means they are robust to outliers), and there are relatively few parameters to tune.\n",
    "<br>Disadvantages: Decision trees are prone to be overfit. However, this can be addressed by ensemble methods like random forests or boosted trees.\n",
    " \n",
    "### 32. What are the advantages and disadvantages of neural networks?\n",
    "<br>Advantages: Neural networks (specifically deep NNs) have led to performance breakthroughs for unstructured datasets such as images, audio, and video. Their incredible flexibility allows them to learn patterns that no other ML algorithm can learn.\n",
    "<br>Disadvantages: However, they require a large amount of training data to converge. It's also difficult to pick the right architecture, and the internal \"hidden\" layers are incomprehensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. Explain Principle Component Analysis (PCA).\n",
    "PCA is a method for transforming features in a dataset by combining them into uncorrelated linear combinations.\n",
    "\n",
    "These new features, or principal components, sequentially maximize the variance represented (i.e. the first principal component has the most variance, the second principal component has the second most, and so on).\n",
    "As a result, PCA is useful for dimensionality reduction because you can set an arbitrary variance cutoff.\n",
    "\n",
    "### 34. Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?\n",
    "AUROC is robust to class imbalance, unlike raw accuracy.\n",
    "For example, if you want to detect a type of cancer that's prevalent in only 1% of the population, you can build a model that achieves 99% accuracy by simply classifying everyone has cancer-free.\n",
    " \n",
    "### 35. Why are ensemble methods superior to individual models?\n",
    "They average out biases, reduce variance, and are less likely to overfit.\n",
    "There's a common line in machine learning which is: \"ensemble and get 2%.\"\n",
    "This implies that you can build your models as usual and typically expect a small performance boost from ensembling.\n",
    " \n",
    "### 36. Explain bagging.\n",
    "Bagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided into multiple subsets through resampling.\n",
    "Then, each subset is used to train a model, and the final predictions are made through voting or averaging the component models.\n",
    "Bagging is performed in parallel.\n",
    "\n",
    "### 37.  What is ‘Overfitting’ in Machine learning?\n",
    "In machine learning, when a statistical model describes random error or noise instead of \n",
    "underlying relationship ‘overfitting’ occurs.  When a model is excessively complex, \n",
    "overfitting is normally observed, because of having too many parameters with respect to the \n",
    "number of training data types. The model exhibits poor performance which has been overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38. What are the different Algorithm techniques in Machine Learning?\n",
    "A. The different types of techniques in Machine Learning are\n",
    "a)      Supervised Learning\n",
    "b)      Unsupervised Learning\n",
    "c)       Semi-supervised Learning\n",
    "\n",
    "### 39. What are the five popular algorithms of Machine Learning?\n",
    "a)      Decision Trees<br>\n",
    "b)      Neural Networks (back propagation)<br>\n",
    "c)       Probabilistic networks<br>\n",
    "d)      Nearest Neighbor<br>\n",
    "e)      Support vector machines<br>\n",
    "\n",
    "### 40. Explain what is the function of ‘Unsupervised Learning’?\n",
    "a)      Find clusters of the data <br>\n",
    "b)      Find low-dimensional representations of the data <br>\n",
    "c)       Find interesting directions in data <br>\n",
    "d)      Interesting coordinates and correlations <br>\n",
    "e)      Find novel observations/ database cleaning <br>\n",
    "\n",
    "### 41.   Explain what is the function of ‘Supervised Learning’?\n",
    "a)      Classification <br>\n",
    "b)      Speech recognition <br>\n",
    "c)       Regression <br>\n",
    "d)      Predict time series<br>\n",
    "\n",
    "### 42. What is ensemble learning?\n",
    "To solve a particular computational program, multiple models such as classifiers or experts are strategically generated and combined. This process is known as ensemble learning.\n",
    "\n",
    "Ensemble learning is used to improve the classification, prediction, function approximation etc of a model.\n",
    "\n",
    "Ensemble learning is used when you build component classifiers that are more accurate and independent from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.  Bagging is a method in ensemble for improving unstable estimation or classification schemes.  While boosting method are used sequentially to reduce the bias of the combined model.  Boosting and Bagging both can reduce errors by reducing the variance term.\n",
    "\n",
    "### 44. What is bias-variance decomposition of classification error in ensemble method?\n",
    "The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function.  The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
