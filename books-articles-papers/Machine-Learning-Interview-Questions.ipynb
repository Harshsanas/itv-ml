{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the difference between AI and ML?\n",
    "\n",
    "AI stands for Artificial Intelligence and ML stands for Machine Learning. These are the two hot buzz words in today’s era. This is the decade of big data. What makes a big data special is the humongous data set which are provided for analysis. These data sets cannot be processed using rudiment database management system on the first place.\n",
    "\n",
    "AI was introduced in 1960 with the purpose to help human by creating computer programs which can replicate human actions in the same way for a given situation. It was supposed to have intelligence at par with human.\n",
    "\n",
    "Machine learning on the other hand is just one aspect of AI. Given a scenario, a machine should be able to learn prediction. Given a data set the machine should be able to learn from the trend/past behavior and predict the answer. If I give dataset for Man of the Match in a set of cricket matches, then the machine should be able to come with the right candidate for Man of The Match in the coming matches. The better the learning curve of the model, the better is the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is deep learning?\n",
    "\n",
    "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is gradient descent?\n",
    "\n",
    "Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on a convex function, that tweaks it’s parameters iteratively to minimize a given function to its local minimum.\n",
    "\n",
    "It is simply used to find the values of a functions parameters (coefficients) that minimize a cost function as far as possible.\n",
    "\n",
    "“A gradient measures how much the output of a function changes if you change the inputs a little bit.” — Lex Fridman (MIT)\n",
    "\n",
    "It simply measures the change in all weights with regard to the change in error. You can also think of a gradient as the slope of a function. The higher the gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning. Said it more mathematically, a gradient is a partial derivative with respect to its inputs.\n",
    "\n",
    "Gradient Descent can be thought of climbing down to the bottom of a valley, instead of climbing up a hill. This is because it is a minimization algorithm that minimizes a given function.\n",
    "\n",
    "The equation below describes what Gradient Descent does: \"b“ describes the next position of our climber, while \"a“ represents his current position. The minus sign refers to the minimization part of gradient descent. The \"gamma“ in the middle is a waiting factor and the gradient term ( Δf(a) ) is simply the direction of the steepest descent.\n",
    "\n",
    "![Fig](imgs/img_001.png)\n",
    "\n",
    "[Gradient Descent Explained](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What is backpropagation?\n",
    "\n",
    "Backpropagation is a technique which helps us in tuning our model by changing the parameters involved in the model until a better accuracy is achieved.\n",
    " \n",
    "In layman term, suppose you have to create and ensemble model which needs to have weight for every algorithm which you have selected for the model. Backpropagation will just increase/decrease both the model weightage as well as the parameter of each model to achieve a better accuracy, precision, recall and F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What is precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision comes up from the English word precise which means accurate.\n",
    " \n",
    "Take an example – Your mother has gifted you 100 things in the past 10 years. She loves you and you love her back. One fine day she comes up and asks you “Do you remember all my 100 gifts I gave you?”\n",
    " \n",
    "You can’t run away, but you can try recalling. With God’s grace you were able to remember 80 items, not good but you did decent.\n",
    "\n",
    "Your recall rate here is 80%, all the items you remembered were correct, so the precision here will be 100%.\n",
    "\n",
    "p is the number of correct postive results divided by total number of postive results.\n",
    "\n",
    "Take another example where you had 3 girlfriends(highly hypothetical), and the current girlfriend asked you to recall all the 10 gifts she had given you in the past 2 years.\n",
    " \n",
    "You tried and answered all 10, but to your surprise only 7 were from this girlfriend..oopss!!\n",
    " \n",
    "So, your recall percentage is 100 but the precision is 70%\n",
    " \n",
    "r is the number of correct positive results divided by the number of correct positive results and incorrect negative results (or postive results that should have been returned)\n",
    "\n",
    "![Fig](imgs/img_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is F-score? Is it like the perfect metric to judge the performance of a model?\n",
    "\n",
    "A.   It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results, and r is the number of correct positive results divided by the number of positive results that should have been returned. The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What is the most important parameter to judge a model?\n",
    "A. To be frank, there are many. You can’t judge any model just on accuracy, precision, recall, Area Under Curve, Sensitivity, specificity, F-score. The final model or the ensemble model has to perform equally well on multiple parameters.\n",
    " \n",
    "Ideally a model with good accuracy in train data set and almost comparable accuracy in validation data set is good. It should have a high Area Under Curve and F-score\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What are the type of datasets?\n",
    "A. Basically there is just dataset. Say suppose, you have the dataset for diabetes patient. Your dataset contains Height, weight, insulin level, age, diabetes(true or false)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. What is train and validation dataset?\n",
    "A. Say your dataset has 10000 rows. Now you have 10000 rows, you want to build a model. You take all the rows and feed in your model. Your accuracy and everything is very good. But how will you validate the model?\n",
    " \n",
    "To answer this question, we divide the 10000 rows into two parts – Train and Validation in the ratio 70:30 or 80:20 .\n",
    " \n",
    "You can build your model with 70 or 80 % of the data and the rest 20 % will be there to validate your model.\n",
    "\n",
    "Example – Your diabetes train data set got an accuracy of 90% but when you tested this model with the validate dataset the accuracy was ~50. This means that model will not work well for other datasets.\n",
    " \n",
    "If the accuracy is almost the same as train dataset then it’s a decent model. Again one have to look for good F-score and AUC.\n",
    " \n",
    "To keep it short, a good model depends on many parameters which varies from one problem statement to another\n",
    "If the accuracy is almost the same as train dataset then it’s a decent model. Again one have to look for good F-score and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Explain an ROC curve.\n",
    " \n",
    "A. Receiver Operating Curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).\n",
    " \n",
    "For a binary classifier, the ROC curve plots the true positive rate versus the fall positive rate, over a varying threshold. For example, say you developed some test for a disease. You would obtain some data on patients with the disease, and control subjects, then fit a model on it; the purpose of the model is to predict disease status from some set of variables. The true positive rate is subjects who have the disease who are correctly identified as having it, and the false positive rate is subjects who don’t have the disease, but are identified as having it (by your model).\n",
    "\n",
    "There is some threshold for designating that a person has the disease. Say at or above a test value of .5, you consider that enough evidence to denote that person as having the disease. As you vary this threshold, the true and false positive rates change. This is the ROC curve.\n",
    "The most concise interpretation in my opinion, is the AUC, the area under the (ROC) curve. It ranges from 0.5 (classification at random) to 1.0 (perfect classification). You can generally think of the AUC as the probability that your model will correctly classify a given subject into one of the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. What’s your favourite algorithm, and can you explain it to me in less than a minute?\n",
    "\n",
    "A. This type of question tests your understanding of how to communicate complex and technical nuances with poise and the ability to summarize quickly and efficiently.\n",
    "You can take an example to show the importance of the algorithm of your choice. Given a chance I would start with linear and logistic and then move to Boosting algorithms like XGBoost or LightGBM or Random forest using a classification problem. \n",
    "\n",
    "There was one problem where we had to determine whether a given patient will be affected by some disease or not. It was a classic classification problem. I started with Linear model, had a decent accuracy in logistic regression, awesome AUC in random forest. So I finally decided to settle with an ensemble model of XGB and Naïve Bayes.\n",
    "\n",
    "Given a chance you should also tell all your favourite algorithm and later boil down to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. What’s the difference between Type I and Type II error?\n",
    "\n",
    "A. Interviewer might try to pin you down with some confusing term, and this one being one of them. Type 1 and Type 2 error is nothing but false positive and false negative.\n",
    "Type 1 means claiming an event has happened but it hasn’t.\n",
    "Type 2 means claiming that nothing has happened when in fact then event has already took place\n",
    "A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. How is a decision tree pruned?\n",
    "A. Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.\n",
    "\n",
    "Reduced error pruning is perhaps the simplest version: replace each node.\n",
    "\n",
    "If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Which is more important to you– model accuracy, or model performance?\n",
    "\n",
    "A. This question tests your grasp of the nuances of machine learning model performance! Machine learning interview questions often look towards the details. There are models with higher accuracy that can perform worse in predictive power — how does that make sense?<br>\n",
    "\n",
    "Well, it has everything to do with how model accuracy is only a subset of model performance, and at that, a sometimes misleading one.<br>\n",
    "\n",
    "For example, if you wanted to detect fraud in a massive dataset with a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast minority of cases were fraud. However, this would be useless for a predictive model — a model designed to find fraud that asserted there was no fraud at all! Questions like this help you demonstrate that you understand model accuracy isn’t the be-all and end-all of model performance.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. How would you handle an imbalanced dataset?\n",
    "A. An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:\n",
    "1. Collect more data to even the imbalances in the dataset.\n",
    "2. Resample the dataset to correct for imbalances.\n",
    "3. Try a different algorithm altogether on your dataset. Many algorithms provide scaling of samples.\n",
    "What’s important here is that you have a keen sense for what damage an unbalanced dataset can cause, and how to balance that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Name an example where ensemble techniques might be useful.\n",
    "A. Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by small changes in the training data). \n",
    "You could list some examples of ensemble methods, the problem of identifying diabetic patient is one such example where you need to have an ensemble method to have a better accuracy and f-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. How do you ensure you’re not overfitting with a model?\n",
    "A. This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations.\n",
    "There are three main methods to avoid overfitting:\n",
    "\n",
    "1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.\n",
    "\n",
    "2- Use cross-validation techniques such as k-folds cross-validation.\n",
    "\n",
    "3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Why is sometimes 96% accuracy is also bad?\n",
    "A. This is really important to understand that accuracy is not the only measure of success for a model. Suppose you are making a model for cancer patients, there an accuracy of even 96% is not that acceptable because if 4% of the people who have been labeled as not infected by cancer later found to be suffering from cancer, then the accuracy of the model won’t matter that much.\n",
    "\n",
    "In this case we are more concerned about sensitivity and specificity of the model\n",
    "Sensitivity is true positive rate and specificity is true negative rate\n",
    " \n",
    "As a remedy we can take the following step to have a better sample for creating the model:-\n",
    " \n",
    "a. Try using under sampling or oversampling method to create your dataset\n",
    "b. We can assign weight to classes such that the minority classes gets larger weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Can you tell me how sensitivity and specificity are inversely proportional to each other?\n",
    "A. “Inversely proportional” means that as one goes up, the other goes down.\n",
    "\n",
    "“Sensitivity” is the proportion of positives that are correctly identified as positives.\n",
    "\n",
    "“Specificity” is the proportion of negatives that are correctly identified as negatives.\n",
    "\n",
    "So, suppose you have some test for a disease and your test gives a numerical score from 0 to 100 with a higher score indicating greater likelihood of disease. You have to set a cutoff (or, possibly, several, or maybe something fuzzy, but let’s keep it simple).\n",
    "If you say “only people with a score of 100 have the disease” then you will have very good sensitivity but horrible specificity.\n",
    "If you say “anyone with a score higher than 1 has the disease” then you will have horrible sensitivity and great specificity.\n",
    "Where to set the cut off depends on the exact shape of the ROC and on the cost of a false negative vs. a false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv",
   "language": "python",
   "name": "itv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
