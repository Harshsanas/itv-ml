{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we discuss two resampling methods - cross validation and bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error vs Training Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error is the average error that results from using a\n",
    "statistical learning method to predict the response on a new\n",
    "observation, one that was not used in training the method.<br>\n",
    "In contrast, the training error can be easily calculated by\n",
    "applying the statistical learning method to the observations\n",
    "used in its training.<br>\n",
    "But the training error rate often is quite different from the\n",
    "test error rate, and in particular the former can\n",
    "dramatically underestimate the latter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to estimate test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss ways to estimate error in the test set. We assume we are given 2 sets of data: training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we randomly divide the available set of samples into\n",
    "two parts: a training set and a validation or hold-out set.<br>\n",
    "eg. train_test_split function in sklearn <br>\n",
    "• The model is fit on the training set, and the fitted model is\n",
    "used to predict the responses for the observations in the\n",
    "validation set.<br>\n",
    "• The resulting validation-set error provides an estimate of\n",
    "the test error. This is typically assessed using MSE in the<br>\n",
    "case of a quantitative response and misclassification rate in\n",
    "the case of a qualitative (discrete) response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the validation estimate of the test error can be highly\n",
    "variable, depending on precisely which observations are<br>\n",
    "included in the training set and which observations are\n",
    "included in the validation set.<br>\n",
    "• In the validation approach, only a subset of the\n",
    "observations — those that are included in the training set<br>\n",
    "rather than in the validation set — are used to fit the\n",
    "model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Widely used approach for estimating test error.<br>\n",
    "• Estimates can be used to select best model, and to give an\n",
    "idea of the test error of the final chosen model.<br>\n",
    "• Idea is to randomly divide the data into K equal-sized\n",
    "parts. We leave out part k, fit the model to the other<br>\n",
    "K − 1 parts (combined), and then obtain predictions for\n",
    "the left-out kth part.<br>\n",
    "• This is done in turn for each part k = 1, 2, . . . K, and then\n",
    "the results are combined.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the K parts be C1, C2, . . . CK, where Ck denotes the indices of the observations in part k. \n",
    "There are nk observations in part k: if N is a multiple of K, then nk = n/K.<br>\n",
    "• Compute error over K folds <br>\n",
    "• Setting K = n yields n-fold or leave-one out cross-validation (LOOCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrap approach allows us to use a\n",
    "computer to mimic the process of obtaining new data sets,<br>\n",
    "so that we can estimate the variability of our estimate\n",
    "without generating additional samples.<br>\n",
    "• Rather than repeatedly obtaining independent data sets\n",
    "from the population, we instead obtain distinct data sets<br>\n",
    "by repeatedly sampling observations from the original data\n",
    "set with replacement.<br>\n",
    "• Each of these “bootstrap data sets” is created by sampling\n",
    "with replacement, and is the same size as our original<br>\n",
    "dataset. As a result some observations may appear more\n",
    "than once in a given bootstrap data set and some not at all.\n",
    "\n",
    "![Fig](imgs/img_003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• In cross-validation, each of the K validation folds is\n",
    "distinct from the other K − 1 folds used for training: there\n",
    "is no overlap. This is crucial for its success.<br>\n",
    "• To estimate prediction error using the bootstrap, we could\n",
    "think about using each bootstrap dataset as our training <br>\n",
    "sample, and the original sample as our validation sample.\n",
    "• But each bootstrap sample has significant overlap with the <br>\n",
    "original data. About two-thirds of the original data points\n",
    "appear in each bootstrap sample. <br>\n",
    "• This will cause the bootstrap to seriously underestimate\n",
    "the true prediction error. <br>\n",
    "• The other way around— with original sample = training\n",
    "sample, bootstrap dataset = validation sample— is worse! <br>\n",
    "\n",
    "We can partly fix this problem by only using predictions for\n",
    "those observations that did not (by chance) occur in the <br>\n",
    "current bootstrap sample.\n",
    "\n",
    "Cross Validation is the preferred method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Methods for Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe tree-based methods for regression and\n",
    "classification. <br>\n",
    "• These involve stratifying or segmenting the predictor space\n",
    "into a number of simple regions.<br>\n",
    "• Since the set of splitting rules used to segment the\n",
    "predictor space can be summarized in a tree, these types of<br>\n",
    "approaches are known as decision-tree methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based methods are simple and useful for\n",
    "interpretation.<br>\n",
    "• However they typically are not competitive with the best\n",
    "supervised learning approaches in terms of prediction\n",
    "accuracy.<br>\n",
    "• Hence we also discuss bagging, random forests, and\n",
    "boosting. These methods grow multiple trees which are\n",
    "then combined to yield a single consensus prediction.<br>\n",
    "• Combining a large number of trees can often result in\n",
    "dramatic improvements in prediction accuracy, at the\n",
    "expense of some loss interpretation<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Consider baseball data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_004.png)\n",
    "![Fig](imgs/img_005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hitters data, a regression tree for predicting the log\n",
    "salary of a baseball player, based on the number of years that he,<br>\n",
    "has played in the major leagues and the number of hits that he\n",
    "made in the previous year.<br>\n",
    "• At a given internal node, the label (of the form Xj < tk)\n",
    "indicates the left-hand branch emanating from that split, and <br>\n",
    "the right-hand branch corresponds to Xj ≥ tk. For instance, the\n",
    "split at the top of the tree results in two large branches. The <br>\n",
    "left-hand branch corresponds to Years<4.5, and the right-hand\n",
    "branch corresponds to Years>=4.5.<br>\n",
    "• The tree has two internal nodes and three terminal nodes, or\n",
    "leaves. The number in each leaf is the mean of the response for\n",
    "the observations that fall there.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a decision tree works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represented by boxes, the interior nodes of the decision tree test features. These nodes are connected by edges that specify the possible outcomes of the tests. The training instances are divided into subsets based on the outcomes of the tests. For example, a node might test whether or not the value of a featureexceeds a threshold. The instances that pass the test will follow an edge to the node's right child, and the instances that fail the test will follow an edge to the node's left child. The children nodes similarly test their subsets of the training instances until a stopping criterion is satisfied. In classification tasks, the leaf nodes of the decision tree represent classes. In regression tasks, the values of the response variable for the instances contained in a leaf node may be averaged to produce the estimate for the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a decision tree using an algorithm called Iterative Dichotomiser 3(ID3). Invented by Ross Quinlan, ID3 was one of the first algorithms used to train decision trees. Assume that you are tasked with classifying animals as cats or dogs. Unfortunately, you cannot observe the animals directly and must use only a few attributes of the animals to make your decision. For each animal, you are told whether or not it likes to play fetch, whether or not it is frequently grumpy, and its favorite of three types of food.To classify new animals, the decision tree will examine a feature at each node. The edge it follows to the next node will depend on the outcome of the test. For example, the first node might ask whether or not the animal likes to play fetch. If the animal does, we will follow the edge to the left child node; if not, we will follow the edge to the right child node. Eventually an edge will connect to a leaf node that indicates whether the animal is a cat or a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantify the amount of uncertainty using a measure called entropy.Measured in bits, entropy quantifies the amount of uncertainty in a variable. Entropy is given by the following equation, where n is the number of outcomes and P(xi) is the probability of outcome i. Common values for b are 2, e, and 10. Because the log of a number less than 1 will be negative, the entire sum is negated to return a positive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a single toss of a fair coin has only two outcomes: heads and tails. The probability that the coin will land on heads is 0.5, and the probability that it will land on tails is 0.5. The entropy of the coin toss is equal to the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, only 1 bit is required to represent two equally probable outcomes: heads or tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the entropy of classifying an unknown animal. If an equal number of dogs and cats comprise our animal classification training data and we do not know anything else about the animal, the entropy of the decision is equal to 1. All we know is that the animal could be either a cat or a dog; like the fair coin toss, both outcomes are equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data contains six dogs and eight cats. If we do not know anything else about the unknown animal, the entropy of the decision is given by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root node tests the plays fetchfeature. Recall that we converted the this Boolean explanatory variable to a binary-valued feature. Training instances for which plays fetchis equal to zero follow the edge to the root's left child, and training instances for animals that do play fetch follow the edge to the root's right child node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left node contains a subset of the training data with seven cats and two dogs that do not like to play fetch. The entropy at this node is given by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right child contains a subset with one cat and four dogs that do like to play fetch. The entropy at this node is given by:\n",
    "![Fig](imgs/img_014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 is not the only algorithm can be used to train decision trees. C4.5 is a modified version of ID3 that can be used with continuous explanatory variables and can accommodate missing values for features. C4.5 can alsoprune trees. Pruning reduces the size of a tree by replacing branches that classify few instances with leaf nodes. Used by scikit-learn's implementation of decision trees, CART is another learning algorithm that supports pruning. Now that we have an understanding of the ID3 algorithm and an appreciation for the labor it automates, we will discuss building decision tress with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for the animals that prefer cat food resulted in one subset with six cats, zero dogs, and zero bits of entropy, and another subset with two cats, six dogs, and 0.811 bits of entropy. How can we measure which of these tests reduced our uncertainty about the classification the most? Averaging the entropies of the subsets may seem to be an appropriate measure of the reduction in entropy. However, selecting the test that produces the subsets with the lowest average entropy can produce a sub-optimal tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we will measure the reduction in entropy using a metric called information gain. Calculated with the following equation, information gain is the difference between the entropy of the parent node, H(T), and the weighted average of the children nodes' entropies. T is the set of instances, and a is the feature under test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_016.png)\n",
    "![Fig](imgs/img_017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we built a decision tree by creating nodes that produced the greatest information gain. Another common heuristic for learning decision trees is Gini impurity, which measures the proportions of classes in a set. Gini impurity is given by the following equation, where j is the number of classes, t is the subset of instances for the node, and P(i|t) is the probability of selecting an element of class i from the node's subset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/img_018.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, Gini impurity is 0 when all the elements of the set are the same class, as the probability of selecting an element of that class is equal to 1. Like entropy, Gini impurity is greatest when each class has an equal probability of being selected. The maximum value of Gini impurity depends on the number of possible classes and is given by the following equation:\n",
    "\n",
    "Gini_max = 1 - 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv",
   "language": "python",
   "name": "itv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
