{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative variables take values in an unordered set C, such as: eye color ∈ {black, brown, blue, green}\n",
    "    email∈ {spam, not-spam}\n",
    "    \n",
    "• Given a feature vector X and a qualitative response Y taking values in the set C, the classification task is to <br>build a function C(X) that takes as input the feature vector X and predicts its value for Y ; i.e. C(X) ∈ C. <br>\n",
    "• Often we are more interested in estimating the probabilities that X belongs to each category in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case - Credit Card Default\n",
    "\n",
    "![Fig](imgs/lin_reg_024.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose for the Default classification task that we code <br>\n",
    "Y = 0 if No 1 if Yes. <br>\n",
    "\n",
    "Can we simply perform a linear regression of Y on X and classify as Yes if Y-hat > 0.5? <br>\n",
    "\n",
    "• In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to linear<br>\n",
    "discriminant analysis which we discuss later. <br>\n",
    "\n",
    "• Since in the population E(Y |X = x) = Pr(Y = 1|X = x), we might think that regression is perfect for this task.\n",
    "\n",
    "However, linear regression might produce probabilities less than zero or bigger than one. Logistic regression is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.<br>\n",
    "\n",
    "Y = 1 if stroke; 2 if drug overdose; 3 if epileptic seizure.<br>\n",
    "\n",
    "This coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.<br>\n",
    "\n",
    "Linear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write p(X) = Pr(Y = 1|X) for short and consider using balance to predict default. Logistic regression uses the form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_025.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e ≈ 2.71828 is a mathematical constant [Euler’s number.])\n",
    "It is easy to see that no matter what values β0, β1 or X take, p(X) will have values between 0 and 1.\n",
    "A bit of rearrangement gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_026.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This monotone transformation is called the log odds or logit transformation of p(X). (by log we mean natural log: ln.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use maximum likelihood to estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_027.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This likelihood gives the probability of the observed zeros and ones in the data. We pick β0 and β1 to maximize the likelihood of the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, upon running the logisitic regression on the credit card default problem we get the following -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated probabilty of default with a balance of $1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_029.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated probabilty of default with a balance of $2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_030.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with several variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_031.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the approach is to model the distribution of X in each of\n",
    "the classes separately, and then use Bayes theorem to flip things\n",
    "around and obtain Pr(Y |X).<br>\n",
    "When we use normal (Gaussian) distributions for each class,\n",
    "this leads to linear or quadratic discriminant analysis.<br>\n",
    "However, this approach is quite general, and other distributions\n",
    "can be used as well. We will focus on normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Theorem\n",
    "\n",
    "![Fig](imgs/lin_reg_032.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can write this slightly differently -\n",
    "\n",
    "![Fig](imgs/lin_reg_033.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• fk(x) = Pr(X = x|Y = k) is the density for X in class k.\n",
    "Here we will use normal densities for these, separately in\n",
    "each class.<br>\n",
    "• πk = Pr(Y = k) is the marginal or prior probability for\n",
    "class k.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Discriminant Analysis\n",
    "\n",
    "When the classes are well-separated, the parameter\n",
    "estimates for the logistic regression model are surprisingly\n",
    "unstable. Linear discriminant analysis does not suffer from\n",
    "this problem.<br>\n",
    "• If n is small and the distribution of the predictors X is\n",
    "approximately normal in each of the classes, the linear\n",
    "discriminant model is again more stable than the logistic\n",
    "regression model.<br>\n",
    "• Linear discriminant analysis is popular when we have more\n",
    "than two response classes, because it also provides\n",
    "low-dimensional views of the data.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Density has the form -\n",
    "\n",
    "![Fig](imgs/lin_reg_034.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here µk is the mean, and σ2k the variance (in class k). \n",
    "We will assume that all the σk = σ are the same.<br>\n",
    "Plugging this into Bayes formula, we get a rather complex\n",
    "expression for pk(x) = Pr(Y = k|X = x):<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_035.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify at the value X = x, we need to see which of the\n",
    "pk(x) is largest. Taking logs, and discarding terms that do not\n",
    "depend on k, we see that this is equivalent to assigning x to the\n",
    "class with the largest discriminant score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_036.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that δk(x) is a linear function of x.\n",
    "If there are K = 2 classes and π1 = π2 = 0.5, then one can see that the decision boundary is at<br>\n",
    "x = (µ1 + µ2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "![Fig](imgs/lin_reg_037.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have estimates ˆδk(x), we can turn these into estimates for class probabilities:\n",
    "    \n",
    "So classifying to the largest ˆδk(x) amounts to classifying to the class for which Pr( c Y = k|X = x) is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig](imgs/lin_reg_038.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a two-class problem, one can show that for LDA\n",
    "\n",
    "![Fig](imgs/lin_reg_039.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it has the same form as logistic regression.\n",
    "\n",
    "The difference is in how the parameters are estimated.\n",
    "• Logistic regression uses the conditional likelihood based on\n",
    "Pr(Y |X) (known as discriminative learning). <br>\n",
    "• LDA uses the full likelihood based on Pr(X, Y ) (known as\n",
    "generative learning). <br>\n",
    "• Despite these differences, in practice the results are often\n",
    "very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is very popular for classification,\n",
    "especially when K = 2.<br>\n",
    "• LDA is useful when n is small, or the classes are well\n",
    "separated, and Gaussian assumptions are reasonable. Also\n",
    "when K > 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itv",
   "language": "python",
   "name": "itv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
